{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim: predicting the selling price of the car based on various features of the cars, including the present price of the cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the exploratory data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute '_unpickle_block' on <module 'pandas._libs.internals' from 'C:\\\\Users\\\\acer\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\_libs\\\\internals.cp39-win_amd64.pyd'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19404/3241109267.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_eda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\acer\\\\Desktop\\\\Car-Price-Prediction\\\\models\\\\ExploratoryDataAnalysis.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_eda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute '_unpickle_block' on <module 'pandas._libs.internals' from 'C:\\\\Users\\\\acer\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\_libs\\\\internals.cp39-win_amd64.pyd'>"
     ]
    }
   ],
   "source": [
    "df_eda = pickle.load(open('C:\\\\Users\\\\acer\\\\Desktop\\\\Car-Price-Prediction\\\\models\\\\ExploratoryDataAnalysis.pkl','rb'))\n",
    "df_eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#independent features and dependent features\n",
    "X=df_eda.drop(columns='selling_price',axis=1)\n",
    "y=df_eda['selling_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standarization of the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#machine learning algorithms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression model\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)#training the algorithm\n",
    "\n",
    "#getting coefficients and intercepts\n",
    "print('coefficients:',lin_reg.coef_)\n",
    "print('intercept:',lin_reg.intercept_)\n",
    "\n",
    "#predicting on the test data\n",
    "y_pred_lin=lin_reg.predict(X_test)\n",
    "\n",
    "#compare the actual output values and predicted values\n",
    "df = pd.DataFrame({'Actual':y_test, 'Predicted':y_pred_lin})\n",
    "print(df.reset_index(inplace=True,drop=True))\n",
    "\n",
    "#showing the difference between the actual and predicted values\n",
    "df1=df.head(25)\n",
    "print(df1.plot(kind='bar', figsize=(15,5)))\n",
    "\n",
    "#calculate the accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "print('r2_score:',metrics.r2_score(y_test,y_pred_lin))\n",
    "print('Mean Absolute Error:',metrics.mean_absolute_error(y_test,y_pred_lin))\n",
    "print('Mean Squared Error:',metrics.mean_squared_error(y_test,y_pred_lin))\n",
    "print('Root Mean Square Error:',np.sqrt(metrics.mean_squared_error(y_test,y_pred_lin)))\n",
    "\n",
    "#or\n",
    "print('accuracy score of training data:',lin_reg.score(X_train,y_train))\n",
    "print('accuracy score of testing data:',lin_reg.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc{'figure.facecolor':'lightblue'})\n",
    "plt.scatter(y_test,y_pred_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a LR model using statsmodels Ordinary least-squares (OLS) models\n",
    "#!pip install statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "y=df_eda['selling_price']\n",
    "X=df_eda.drop(['selling_price'],axis=1)\n",
    "X_constant=sm.add_constant(X)\n",
    "model=sm.OLS(y,X_constant).fit()\n",
    "model.predict(X_constant)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularized Regression\n",
    "1. Ridge Regression\n",
    "Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients.\n",
    "\n",
    "Loss function = OLS + alpha * summation (squared coefficient values)\n",
    "\n",
    "In the above loss function, alpha is the parameter we need to select. A low alpha value can lead to over-fitting, whereas a high alpha value can lead to under-fitting.\n",
    "\n",
    "Instead of arbitrarily choosing alpha value ,it would be better to use cross-validation to choose the tuning parameter alpha. We can do this using the cross-validated ridge regression function, RidgeCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building ridgeregression model\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "\n",
    "alphas=10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "ridge_cv= RidgeCV(alphas=alphas,normalize=True)\n",
    "ridge_cv.fit(X_train,y_train)\n",
    "print(ridge_cv.alpha_)\n",
    "\n",
    "#ridge regression L2 regularization\n",
    "ridge=Ridge(alpha=ridge_cv.alpha_,normalize=True)\n",
    "ridge.fit(X_train,y_train)\n",
    "\n",
    "print('Root Mean Squared Error:',np.sqrt(metrics.mean_squared_error(y_test,ridge.predict(X_test))))\n",
    "print('r2_score:',r2_score(y_test,ridge.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge regression L2 regularization\n",
    "ridge=Ridge(alpha=ridge_cv.alpha_,normalize=True)\n",
    "ridge.fit(X_train,y_train)\n",
    "y_predr=ridge.predict(X_test)\n",
    "\n",
    "print('Root Mean Squared Error:',np.sqrt(metrics.mean_squared_error(y_test,y_predr)))\n",
    "print('r2_score:',r2_score(y_test,y_predr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Lasso Regression\n",
    "Lasso regression, or the Least Absolute Shrinkage and Selection Operator, is also a modification of linear regression. In Lasso, the loss function is modified to minimize the complexity of the model by limiting the sum of the absolute values of the model coefficients (also called the l1-norm).\n",
    "\n",
    "The loss function for Lasso Regression can be expressed as below:\n",
    "\n",
    "Loss function = OLS + alpha * summation (absolute values of the magnitude of the coefficients)\n",
    "\n",
    "We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. In order to fit a lasso model, we'll use the Lasso() function; however, this time we'll need to include the argument max_iter = 10000. Other than that change, we proceed just as we did in fitting a ridge model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building lasso regression model\n",
    "from sklearn.linear_model import Lasso,LassoCV\n",
    "#without alpha and cv parameters\n",
    "lasso=Lasso(max_iter=10000,normalize=True)\n",
    "lasso.fit(X_train,y_train)\n",
    "y_predl=lasso.predict(X_test)\n",
    "print(lasso.coef_)\n",
    "\n",
    "#with CV\n",
    "lasso_cv=LassoCV(alphas=None,cv=10,max_iter=100000,normalize=True)\n",
    "lasso_cv.fit(X_train,y_train)\n",
    "\n",
    "lasso.set_params(alpha=lasso_cv.alpha_)\n",
    "lasso.fit(X_train,y_train)\n",
    "y_predll=lasso.predict(X_test)\n",
    "print(lasso.coef_)\n",
    "print('r2_score:',r2_score(y_test,y_predll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "dtr=DecisionTreeRegressor(max_depth=60, min_samples_leaf=10,min_samples_split=10)\n",
    "dtr.fit(X_train,y_train)\n",
    "\n",
    "y_pred_dtr=dtr.predict(X_test)\n",
    "\n",
    "print('r2_score:',metrics.r2_score(y_test,y_pred_dtr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest Regressor\n",
    "rfr=RandomForestRegressor()\n",
    "rfr.fit(X_train,y_train)\n",
    "y_pred_rf=rfr.predict(X_test)\n",
    "print(metrics.r2_score(y_test,y_pred_rf))\n",
    "\n",
    "#with best prameters\n",
    "rfr_best_model=RandomForestRegressor(n_estimators=300,\n",
    "                                     max_features='sqrt',\n",
    "                                     min_samples_split=10,min_samples_leaf=1,max_depth=30)\n",
    "rfr_best_model.fit(X_train,y_train)\n",
    "y_pred_rfr=rfr_best_model.predict(X_test)\n",
    "print(metrics.r2_score(y_test,y_pred_rfr))\n",
    "#If \"sqrt\", then max_features=sqrt(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "#voting classifier\n",
    "vote = VotingRegressor(estimators=[('LR', lin_reg), ('RFR', rfr), ('DTR', dtr)])\n",
    "vote.fit(X_train, y_train)\n",
    "print(vote.estimators_)\n",
    "y_pred_voting = vote.predict(X_test)\n",
    "\n",
    "#accuracy score\n",
    "score = metrics.r2_score(y_pred_voting, y_test)\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "labels = ['Linear Regression', 'Random Forest Regressor', 'Decision Tree Regressor']\n",
    "\n",
    "for classifier, label in zip([lin_reg, rfr, dtr], labels):\n",
    "    scores = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=cv)\n",
    "    print('accuracy:',scores.mean(), scores.std(), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the observations, we can coclude that Random Forest Regressor is the best model among other models for our car price prediction.\n",
    "\n",
    "Random Forest is an ensemble learning based regression model. It uses a model called decision tree, multiple decision trees to generate the ensemble model which collectively produces a prediction.\n",
    "\n",
    "The benefit of this model is that the trees are produced in parallel and are relatively uncorrelated, producing good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\acer\\\\Desktop\\\\Car-Price-Prediction\\\\models\\\\best_model_car_prediction.pkl', 'wb') as file:\n",
    "    pickle.dump(rfr_best_model, file)\n",
    "\n",
    "#loading a pickle file from models directory\n",
    "model_file=pickle.load(open('C:\\\\Users\\\\acer\\\\Desktop\\\\Car-Price-Prediction\\\\models\\\\best_model_car_prediction.pkl', 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
